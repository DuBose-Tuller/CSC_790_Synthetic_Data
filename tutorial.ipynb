{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce2da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install DataSynthesizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe2cfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install DataSynthesizer jupyter ipython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdc463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate synthetic tabular data using DataSynthesizer\n",
    "\"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "from DataSynthesizer.DataDescriber import DataDescriber\n",
    "from DataSynthesizer.DataGenerator import DataGenerator\n",
    "\n",
    "\n",
    "def generate_synthetic_data(input_file, output_file, num_rows, mode='independent',\n",
    "                           categorical_threshold=30, histogram_bins=10, \n",
    "                           max_parents=2, categorical_cols=None):\n",
    "    \"\"\"\n",
    "    Generate synthetic data from input CSV file.\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to input CSV file\n",
    "        output_file: Path to save synthetic CSV\n",
    "        num_rows: Number of synthetic rows to generate\n",
    "        mode: 'independent' or 'correlated'\n",
    "        categorical_threshold: Max unique values to treat as categorical\n",
    "        histogram_bins: Number of bins for numeric distributions\n",
    "        max_parents: Max parents in Bayesian network (correlated mode only)\n",
    "        categorical_cols: List of column names to force as categorical\n",
    "    \"\"\"\n",
    "    # Use temp description file\n",
    "    description_file = f'.tmp_description_{os.getpid()}.json'\n",
    "    \n",
    "    try:\n",
    "        # Build categorical attributes dict\n",
    "        categorical_attributes = {}\n",
    "        if categorical_cols:\n",
    "            for col in categorical_cols:\n",
    "                categorical_attributes[col] = True\n",
    "        \n",
    "        # Describe the dataset\n",
    "        describer = DataDescriber(\n",
    "            category_threshold=categorical_threshold,\n",
    "            histogram_bins=histogram_bins\n",
    "        )\n",
    "        \n",
    "        if mode == 'independent':\n",
    "            describer.describe_dataset_in_independent_attribute_mode(\n",
    "                dataset_file=input_file,\n",
    "                attribute_to_is_categorical=categorical_attributes,\n",
    "                attribute_to_is_candidate_key={}\n",
    "            )\n",
    "        elif mode == 'correlated':\n",
    "            describer.describe_dataset_in_correlated_attribute_mode(\n",
    "                dataset_file=input_file,\n",
    "                epsilon=0,\n",
    "                k=max_parents,\n",
    "                attribute_to_is_categorical=categorical_attributes\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid mode: {mode}. Use 'independent' or 'correlated'\")\n",
    "        \n",
    "        describer.save_dataset_description_to_file(description_file)\n",
    "        \n",
    "        # Generate synthetic dataset\n",
    "        generator = DataGenerator()\n",
    "        \n",
    "        if mode == 'independent':\n",
    "            generator.generate_dataset_in_independent_mode(num_rows, description_file)\n",
    "        else:  # correlated\n",
    "            generator.generate_dataset_in_correlated_attribute_mode(num_rows, description_file)\n",
    "        \n",
    "        generator.save_synthetic_data(output_file)\n",
    "        \n",
    "    finally:\n",
    "        # Clean up temp description file\n",
    "        if os.path.exists(description_file):\n",
    "            os.remove(description_file)\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Generate synthetic tabular data',\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "        epilog=\"\"\"\n",
    "Examples:\n",
    "  # Basic usage\n",
    "  python synthetic_generator.py input.csv -o synthetic.csv -n 1000\n",
    "  \n",
    "  # Specify categorical columns\n",
    "  python synthetic_generator.py data.csv -o synth.csv -n 500 -c gender smokes is_fit\n",
    "  \n",
    "  # Use correlated mode\n",
    "  python synthetic_generator.py data.csv -o synth.csv -n 1000 -m correlated\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    parser.add_argument('input_file', help='Input CSV file path')\n",
    "    parser.add_argument('-o', '--output', required=True, help='Output CSV file path')\n",
    "    parser.add_argument('-n', '--num-rows', type=int, default=1000,\n",
    "                       help='Number of synthetic rows to generate (default: 1000)')\n",
    "    parser.add_argument('-m', '--mode', choices=['independent', 'correlated'],\n",
    "                       default='independent',\n",
    "                       help='Generation mode (default: independent)')\n",
    "    parser.add_argument('-c', '--categorical', nargs='*', metavar='COL',\n",
    "                       help='Column names to treat as categorical')\n",
    "    parser.add_argument('-t', '--threshold', type=int, default=30,\n",
    "                       help='Categorical threshold - columns with â‰¤N unique values '\n",
    "                            'treated as categorical (default: 30)')\n",
    "    parser.add_argument('-b', '--bins', type=int, default=10,\n",
    "                       help='Number of histogram bins for numeric columns (default: 10)')\n",
    "    parser.add_argument('-k', '--max-parents', type=int, default=2,\n",
    "                       help='Max parents in Bayesian network for correlated mode (default: 2)')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Validate input file exists\n",
    "    if not os.path.exists(args.input_file):\n",
    "        parser.error(f\"Input file not found: {args.input_file}\")\n",
    "    \n",
    "    generate_synthetic_data(\n",
    "        input_file=args.input_file,\n",
    "        output_file=args.output,\n",
    "        num_rows=args.num_rows,\n",
    "        mode=args.mode,\n",
    "        categorical_threshold=args.threshold,\n",
    "        histogram_bins=args.bins,\n",
    "        max_parents=args.max_parents,\n",
    "        categorical_cols=args.categorical\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaf05bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_synthetic_data(\n",
    "    input_file='data/penguins.csv',\n",
    "    output_file='data/synthetic_penguins.csv',\n",
    "    num_rows=1000,\n",
    "    mode='independent',\n",
    "    categorical_threshold=30,\n",
    "    histogram_bins=10,\n",
    "    max_parents=2,\n",
    "    categorical_cols=['species', 'island']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df40ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleanlab\n",
    "from cleanlab import Datalab\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load original dataset\n",
    "print(\"Loading original dataset...\")\n",
    "original_data = pd.read_csv('data/penguins.csv')\n",
    "        \n",
    "\n",
    "# Clean the data - remove missing values\n",
    "original_data = original_data.replace('.', np.nan)\n",
    "original_data = original_data.dropna()\n",
    "\n",
    "print(f\"Original dataset shape: {original_data.shape}\")\n",
    "print(f\"Columns: {original_data.columns.tolist()}\")\n",
    "print(f\"Target column 'sex' unique values: {original_data['sex'].unique()}\")\n",
    "\n",
    "# Initialize Datalab for near duplicate detection\n",
    "print(\"\\nInitializing Datalab for duplicate detection...\")\n",
    "lab = Datalab(data=original_data, label_name='sex')\n",
    "\n",
    "# Find near duplicates\n",
    "print(\"Finding near duplicates...\")\n",
    "lab.find_issues(issue_types={\"near_duplicates\": {}})\n",
    "\n",
    "# Get near duplicate results\n",
    "near_duplicates = lab.get_issues(\"near_duplicates\")\n",
    "num_duplicates = near_duplicates.sum()\n",
    "\n",
    "print(f\"\\nNumber of near duplicates found: {num_duplicates}\")\n",
    "\n",
    "# Show details if duplicates found\n",
    "if num_duplicates > 0:\n",
    "    duplicate_indices = near_duplicates[near_duplicates].index\n",
    "    print(f\"Indices of near duplicates: {duplicate_indices.tolist()}\")\n",
    "    \n",
    "    # Show the actual duplicate records\n",
    "    print(f\"\\nNear duplicate records:\")\n",
    "    duplicate_records = original_data.loc[duplicate_indices]\n",
    "    print(duplicate_records)\n",
    "    \n",
    "    # Get summary statistics\n",
    "    try:\n",
    "        duplicate_summary = lab.get_issue_summary(\"near_duplicates\")\n",
    "        print(f\"\\nDuplicate detection summary:\")\n",
    "        print(duplicate_summary)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get detailed summary: {e}\")\n",
    "        \n",
    "    # Show percentage of duplicates\n",
    "    duplicate_percentage = (num_duplicates / len(original_data)) * 100\n",
    "    print(f\"\\nPercentage of near duplicates: {duplicate_percentage:.2f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"No near duplicates found in the original dataset.\")\n",
    "    \n",
    "print(f\"\\nDataset quality check completed for {len(original_data)} records.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
