{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce2da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6ca810",
   "metadata": {},
   "source": [
    "Below is a function that uses the DataSynthesizer package to generate $n$ samples of synthetic data. The full module can be found at `utils/gen_datasynthesizer.py`. The libarary also generates a JSON file that describes the distribution of all of the features, which gets used in `correlated` mode. However, we treat it as temporary and clean it up when the code finishes.\n",
    "\n",
    "`independent` mode draws indepdently from each of the distributions of the individual features when sampling. While `correlated` constructs a Bayesian Network to relate the distributions of the features to one another, and the combined distribution that DataSynthesizer samples from correlates with all of the features at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdc463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate synthetic tabular data using DataSynthesizer\n",
    "\"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "from DataSynthesizer.DataDescriber import DataDescriber\n",
    "from DataSynthesizer.DataGenerator import DataGenerator\n",
    "\n",
    "\n",
    "def generate_synthetic_data(input_file, output_file, num_rows, mode='independent',\n",
    "                           categorical_threshold=30, histogram_bins=10, \n",
    "                           max_parents=2, categorical_cols=None):\n",
    "    \"\"\"\n",
    "    Generate synthetic data from input CSV file.\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to input CSV file\n",
    "        output_file: Path to save synthetic CSV\n",
    "        num_rows: Number of synthetic rows to generate\n",
    "        mode: 'independent' or 'correlated'\n",
    "        categorical_threshold: Max unique values to treat as categorical\n",
    "        histogram_bins: Number of bins for numeric distributions\n",
    "        max_parents: Max parents in Bayesian network (correlated mode only)\n",
    "        categorical_cols: List of column names to force as categorical\n",
    "    \"\"\"\n",
    "    # Use temp description file\n",
    "    description_file = f'.tmp_description_{os.getpid()}.json'\n",
    "    \n",
    "    try:\n",
    "        # Build categorical attributes dict\n",
    "        categorical_attributes = {}\n",
    "        if categorical_cols:\n",
    "            for col in categorical_cols:\n",
    "                categorical_attributes[col] = True\n",
    "        \n",
    "        # Describe the dataset\n",
    "        describer = DataDescriber(\n",
    "            category_threshold=categorical_threshold,\n",
    "            histogram_bins=histogram_bins\n",
    "        )\n",
    "        \n",
    "        if mode == 'independent':\n",
    "            describer.describe_dataset_in_independent_attribute_mode(\n",
    "                dataset_file=input_file,\n",
    "                attribute_to_is_categorical=categorical_attributes,\n",
    "                attribute_to_is_candidate_key={}\n",
    "            )\n",
    "        elif mode == 'correlated':\n",
    "            describer.describe_dataset_in_correlated_attribute_mode(\n",
    "                dataset_file=input_file,\n",
    "                epsilon=0,\n",
    "                k=max_parents,\n",
    "                attribute_to_is_categorical=categorical_attributes\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid mode: {mode}. Use 'independent' or 'correlated'\")\n",
    "        \n",
    "        describer.save_dataset_description_to_file(description_file)\n",
    "        \n",
    "        # Generate synthetic dataset\n",
    "        generator = DataGenerator()\n",
    "        \n",
    "        if mode == 'independent':\n",
    "            generator.generate_dataset_in_independent_mode(num_rows, description_file)\n",
    "        else:  # correlated\n",
    "            generator.generate_dataset_in_correlated_attribute_mode(num_rows, description_file)\n",
    "        \n",
    "        generator.save_synthetic_data(output_file)\n",
    "        \n",
    "    finally:\n",
    "        # Clean up temp description file\n",
    "        if os.path.exists(description_file):\n",
    "            os.remove(description_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c137b3",
   "metadata": {},
   "source": [
    "Lets make some synthetic penguins!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaf05bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_synthetic_data(\n",
    "    input_file='data/penguins.csv',\n",
    "    output_file='data/synthetic_penguins.csv',\n",
    "    num_rows=345, # Match dataset size\n",
    "    mode='independent',\n",
    "    categorical_cols=['species', 'island']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e036bf3",
   "metadata": {},
   "source": [
    "From looking at the rows of the output, it's quite easy to see that the synthetic data rows use much more precision than the original data!\n",
    "\n",
    "Next, let's look at the distribution of the synthetic data by seeing what proportion of is is considered a near duplicate by our pipeline from Activity 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1226300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.detect_duplicates import DuplicateDetector\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ece81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.9, 0.95, 0.97, 0.99, 1]   \n",
    "results_orig = [] \n",
    "results_synth = [] \n",
    "\n",
    "for threshold in thresholds:\n",
    "    detector = DuplicateDetector(\n",
    "        similarity_threshold=threshold,\n",
    "        n_clusters=None  # None means use sqrt-based dynamic clustering\n",
    "    )\n",
    "\n",
    "    # Original\n",
    "    result_orig = detector.process_file(\n",
    "        'data/penguins.csv',\n",
    "        # create_deduplicated=True\n",
    "    )\n",
    "\n",
    "    # Synthetic\n",
    "    result_synth = detector.process_file(\n",
    "        'data/synthetic_penguins.csv',\n",
    "        # create_deduplicated=True\n",
    "    )\n",
    "\n",
    "    results_orig.append(result_orig['to_remove'])\n",
    "    results_synth.append(result_synth['to_remove'])\n",
    "\n",
    "results_orig = np.array(results_orig)\n",
    "results_synth = np.array(results_synth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25775b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(thresholds, results_orig, label='Original')\n",
    "plt.scatter(thresholds, results_synth, label='Synthetic')\n",
    "plt.xlabel('Similarity Threshold')\n",
    "plt.ylabel('Number of Duplicates to Remove')\n",
    "plt.title('Duplicate Detection: Original vs Synthetic Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c481ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
